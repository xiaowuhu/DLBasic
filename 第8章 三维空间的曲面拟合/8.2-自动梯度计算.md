
## 8.2 自动梯度计算

在第 7 章中学习的曲线拟合的内容在本章中仍然有效，只不过曲线拟合任务只有一个特征值 $x$ 输入，而曲面拟合任务需要有 $x_1、x_2$ 两个特征值输入，分别代表横纵坐标，如表 8.1.2 所示。

我们不想重复在前面的章节中的标准步骤：定义前向计算、定义损失函数、定义反向传播。因为经过前面的学习，读者应该已经清楚了这些基本的步骤，但难点在于：一旦将来设计的神经网络不是这种简单的形式，而是要很多完成其它更复杂的功能时，仍然需要读者**自己来做反向传播的公式推导工作，这种过程很容易出错**。

所以，神经网络设计的挑战之一就是求导问题，而所有的深度学习框架（framework）都提供了所谓的**自动梯度**（auto-grad）的解决方案。这个名字听上去很先进，但实际上所有的自动梯度工作都是工程师在后台写代码完成的，并不存在魔法。下面我们来看看它的实现方法。

### 8.2.1 基本运算符

<img src="./img/computergraph.png" width=480/>

图 8.2.1 计算图

### 8.2.2 组合运算符

### 8.2.3 优化运算符

### 8.2.4 计算图实例【电子资源】


我们用在第 12 章中要学习的批量归一化网络组件为例进行说明。

$$
\mu_B = \frac{1}{m}\sum_{i=1}^m x_i \tag{8.2.2}
$$

这里的 $x_i$ 表示矩阵 $\mathbf x$ 中的一列特征值。比如在一个批量为 $m=64$、特征值数量 $n=3$ 的 64×3 的矩阵中，$x_i=(x_{i1},x_{i2},x_{i3})$，则 64 个 $x_{i1}$ 会产生一个均值 $\mu_B$，三列特征一共会产生三个 $\mu_B$ 形成一个行向量，用于后面的计算。下式中的 $\sigma^2_B$ 也是一个 1×3 的行向量。

$$
\sigma^2_B = \frac{1}{m} \sum_{i=1}^m (x_i-\mu_B)^2 \tag{8.2.3}
$$

下式中的 $x_i-\mu_B$ 中的 $\mu_B$ 先进行广播（纵向拷贝64份），然后在与 $x_i$ 相减，最终的 $y_i$ 与 $x_i$ 的形状相同，$\mathbf x$ 与 $\mathbf y$ 的形状也相同，在本例中都是 64×3。

$$
y_i = \frac{x_i-\mu_B}{\sqrt{\sigma^2_B + \epsilon}} \tag{8.2.4}
$$

在标准的批量归一化计算中，最后还有一步 $z_i = \gamma \odot y_i + \beta$，由于它和全连接层的方式相同，所以在此省略。下面把式（8.2.2）~（8.2.4）分解成自动梯度计算所需的单步计算步骤及其反向计算式如下：

$$
(\mu_B=\frac{1}{m} \sum_{i=1}^m x_i) \to (\frac{\partial \mu_B}{\partial x_i}=\frac{1}{m})
\tag{8.2.5}
$$

把式（8.2.3）分解成以下三步：

$$
(A=x_i-\mu_B) \to (\frac{\partial A}{\partial \mu_B}=-1, \ \frac{\partial A}{\partial x_i}=1)
\tag{8.2.6}
$$

$$
(B=A^2) \to (\frac{\partial A}{\partial B}=2A)
\tag{8.2.7}
$$

$$
(\sigma^2_B=\frac{1}{m} \sum_{i=1}^m B ) \to (\frac{\partial \sigma^2_B}{\partial B}=\frac{1}{m})
\tag{8.2.8}
$$

把式（8.2.4）分解成以下三步：

$$
(C=\sigma^2_B + \epsilon) \to (\frac{\partial C}{\partial \sigma^2_B}=1)
\tag{8.2.9}
$$

$$
(D=\sqrt{C}) \to (\frac{\partial D}{\partial C}=\frac{1}{2\sqrt{C}})
\tag{8.2.10}
$$

$$
(y_i=\frac{A}{D}) \to (\frac{\partial y_i}{\partial A}=\frac{1}{D},\ \frac{\partial y_i}{\partial D}=-\frac{A}{D^2})
\tag{8.2.11}
$$

由此我们可以画出计算图，如图 8.2.2。

<img src="./img/computergraph_bn.png" width=720/>

图 8.2.2 计算图的前向（实线）和反向（虚线）

图 8.2.2 中有两类计算符：

- 单输入变量，如：均值（$\frac{\sum}{m}$）、乘方（**2）、开方（$\sqrt{C}$）。对于这种运算符，反向图中只需要简单地计算一个导数即可，如式（8.2.5）、（8.2.7）、（8.2.8）、（8.2.10）；
- 双输入变量，如：减法、加法、除法。对于这种运算符，反向图中需要分别对两个变量求偏导，如式（8.2.6）、（8.2.11）。其中，式（8.2.9）是一个特殊情况，因为 $\epsilon$ 是常数 `1e-5`。

单有两个或多个变量参与运算时，反向图的路径需要增加分支。比如，求图 8.2.2 中 $loss$ 对 $x_i$ 的偏导数时，一共有 4 个分支，第一个分支如式（8.2.12）所示：

$$
\frac{\partial loss}{\partial x_i}=\frac{\partial loss}{\partial y_i}\frac{\partial y_i}{\partial A}\frac{\partial A}{\partial x_i}
\tag{8.2.12}
$$

这是因为有前向图计算：$y_i=A/D, A=x_i-\mu_B$。而第二个分支因为有式（8.2.5），所以是：

$$
\frac{\partial loss}{\partial x_i}=\frac{\partial loss}{\partial y_i}\frac{\partial y_i}{\partial A}\frac{\partial A}{\partial \mu_B}\frac{\partial \mu_B}{\partial x_i}
\tag{8.2.13}
$$

第三个和第四个分支在前向式（8.2.11）的除法位置产生：

$$
\frac{\partial loss}{\partial x_i}=\frac{\partial loss}{\partial y_i}\frac{\partial y_i}{\partial D}\frac{\partial D}{\partial C}\frac{\partial C}{\partial \sigma^2_B}\frac{\partial \sigma^2_B}{\partial B}\frac{\partial B}{\partial A}\frac{\partial A}{\partial x_i}
\tag{8.2.14}
$$

$$
\frac{\partial loss}{\partial x_i}=\frac{\partial loss}{\partial y_i}\frac{\partial y_i}{\partial D}\frac{\partial D}{\partial C}\frac{\partial C}{\partial \sigma^2_B}\frac{\partial \sigma^2_B}{\partial B}\frac{\partial B}{\partial A}\frac{\partial A}{\partial \mu_B}\frac{\partial \mu_B}{\partial x_i}
\tag{8.2.15}
$$

而最后的 $\frac{\partial loss}{\partial x_i}$ 由以上四个式子的结果相加而得。如果仔细看反向计算图，可以发现在 $y_i$ 处有两个分支，在 $A$ 处有两个分支，所以一共是 $2×2=4$ 四个反向路径（注意不是 $2+2=4$，因为如果在 $A$ 处有三个分支，则一共是 $2×3=6$ 个路径）。

### 8.2.5 实现步骤

自动梯度计算的实现步骤如下。

#### 1. 实现操作符

以除法为例说明，具体实现如下【代码：common.Layers.py】：

```python
class Div(Operator):
    def __call__(self, x, y):
        return self.forward(x, y)
    # 前向：y_i = A / D
    def forward(self, x, y):
        self.x = x      # 保存变量，在反向时使用
        self.y = y
        z = self.x / self.y # 这里有可能有广播
        return z
    # 反向
    def backward(self, dz):
        # 计算偏导数，式（8.2.11)
        dz_dx = 1 / self.y  # dz/dx = 1/y
        dz_dy = - self.x / (self.y * self.y)  # dz/dy = -x/y^2
        # 计算两个分支的反向传播误差
        dx = dz_dx * dz  
        dy = dz_dy * dz
        # 如果 dz 与 x 的形状不同，则需要把 dz 按行累加变成 x 的形状，形成 dx
        dx = super().sum_derivation(self.x, dz, dx)
        dy = super().sum_derivation(self.y, dz, dy)
        # 反向误差的形状必须与输入值相同
        assert(dx.shape == self.x.shape)
        assert(dy.shape == self.y.shape)
        return dx, dy
```

其中的 `super().sum_derivation()` 的微分累计函数实现如下：

```python
class Operator(object):
    # 前向：假设 x 为 (1X3), y 为 (4X3), 计算时 x 广播，z 为（4X3)
    # 反向：计算 x 分支的回传误差时，需要把 dz 按行累加
    def sum_derivation(self, input, delta_in, delta_out):
        # 两个必须都是数组，没有标量
        if isinstance(input, np.ndarray) and isinstance(delta_in, np.ndarray):
            # shape相同的话则不处理
            # shape不同的话，必须列数相同
            # 输出的尺寸比输入的尺寸大
            if input.shape != delta_in.shape and \
               input.shape[1] == delta_in.shape[1] and \
               input.shape[0] < delta_in.shape[0]: 
                # 传入的误差尺寸比输入的尺寸大，意味着输出的尺寸比输入的尺寸大，有广播
                # 所以需要把传入的误差按行相加，保留列的形状，作为输出
                delta_out_sum = np.sum(delta_out, axis=0, keepdims=True)
                return delta_out_sum
        return delta_out
```

除了实现标准的前向、反向计算以外，还需要注意的是 $x$ 和 $y$ 的尺寸有可能不同，比如在本例中，$x$ 的尺寸为 1×3，$y$ 的尺寸为 4×3，在计算 $z=x/y$ 时，先把 $x$ 自动做广播（broadcast），最后得到 4×3 形状的结果。图 8.2.3 解释了这一过程。

<img src="./img/computergraph_div.png" width=360/>

图 8.2.3 以除法为例的自动梯度计算时的数组形状缩放

在计算反向梯度时，由于传入到 $z$ 的梯度 $dz$ 的形状是 4×3，所以 $dx$ 和 $dy$ 的形状都是 4×3，对 $dy$ 来说没有问题，但是 $x$ 分支的形状本来是 1×3，所以需要把 $dx$ 做一次 `sum(dx, axis=0)` 的按行累加，变成 1×3 的形状再回传，术语叫做**规约求和**（reduce sum）。其理论基础是，虽然 $x$ 的形状是 1×3，但是广播后它参与了 4 次计算得到 $z$，所以反向梯度要累加 4 次。

#### 2. 实现计算图

如图 8.2.3 所示，计算图包括两类组件：计算节点、变量节点。有些计算图只有计算节点，而用连接线（边）来表示变量节点。下面是计算节点的代码片段：

```python
# 计算节点
class compute_node(object):
    def __init__(self, op: layer.Operator, name: str, 
        input: list[variable_node], output: variable_node
    ) -> None:
        self.name: str = name                           # 名字，用于索引
        self.op: layer.Operator = op                    # 操作符
        self.input_vars: list[variable_node] = input    # 输入变量（多个）
        self.output_var: variable_node = output         # 输出变量（假设只有一个）
```

下面是变量节点的代码片段：

```python
# 变量节点
class variable_node(object):
    def __init__(self, name: str, value, is_constant: bool=False) -> None:
        self.name = name                    # 名字，索引
        self.value = value                  # 具体值
        self.grad = np.zeros_like(value)    # 梯度
        self.is_constant = is_constant      # 是否常数
    # 保存前向计算值
    def set_value(self, value) -> None:
        assert(self.value.shape == value.shape)
        self.value = value
    # 保存梯度值
    def set_grad(self, value) -> None:
        #assert(self.grad.shape == value.shape) # 有些情况下不相等
        self.grad += value  # 累加梯度，需要在反向开始之前清零
```

连接计算节点和变量节点的是计算图，代码片段如下：

```python
# 计算图
class compute_graph(object):
    def __init__(self) -> None:
        self.list_operator: list[compute_node] = []   # 按顺序保存计算节点，用于前向计算
        self.list_variable: list[variable_node] = []  # 保存变量节点
        self.dict_name2var = {}                       # 按名字快速找到变量
        self.dict_output_var_name_to_op_node = {}     # 按输出变量名快速找到计算节点，用于反向计算
```

在计算图中，最重要的是前向计算图和反向计算图的建立过程，由于代码较长，不详述，只做简要说明。

- 前向计算图的实现思路：由于计算节点是按顺序保存在列表中的，所以只要按顺序执行这些计算步骤，就可以保证前向图的建立。如果不按顺序保存的话，那么有可能一个计算节点的输入还没有被逻辑上的上一个计算节点的输出准备好就要被计算。

- 反向计算图的实现思路：建立一个栈，里面保存“（计算节点名, 梯度值, 输入变量名）”元数组。出栈，从输出变量名反向找到计算节点，在这个计算节点上做反向计算，得到梯度值，再找到这个计算节点的输入变量，压栈。如果有两个分支的话，会在栈中暂时保存一个分支，这样就可以一步步地从后向前找到所有的路。

详细代码请见【代码：H8_2_AD.py】。

#### 3. 建立模型

在建立计算图之前，需要先有一个神经网络模型，这个模型中所使用的所有操作符都需要在第 1 步中实现。然后定义前向计算代码，这样就能够保证计算节点的顺序是正确的。最后再把变量节点和计算节点都加入到计算图中后，就可以循环调用前向计算和反向计算两个方法来实现神经网络的训练了。

以本例来说，在【代码：H8_2_AD.py】的 `build_graph()` 函数中建立好计算图后，就可以按照下面的步骤进行前向、反向计算了。

```python
graph = model.build_graph()  # 建立计算图
z_autograd = graph.build_forward_graph(x) # 前向计算
delta = z_autograd + 1 # 用 +1 得到示意性误差
dx_autograd = graph.build_backward_graph(delta) # 反向计算
```

前向图的输出如下：

```
开始前向计算:
 原始数据           计算       结果
-------------------------------------
['x'] ->            mean_1 -> mu
['x', 'mu'] ->      sub ->    A
['A'] ->            square -> B
['B'] ->            mean_2 -> sigma
['sigma', 'eps'] -> add ->    C
['C'] ->            sqrt ->   D
['A', 'D'] ->       div ->    y
```
反向图的输出如下，请读者与图 8.2.2 做对比，在图中有虚线箭头进入的变量节点：

```
开始反向计算：
误差     原始计算    结果
----------------------------------
y ->     div ->    ['A', 'D']***    # 压栈 A, D     # 这是第一次压栈
D ->     sqrt ->   ['C']            # 出栈 D, 压栈 C
C ->     add ->    ['sigma', 'eps'] # 出栈 C, 压栈 sigma, eps
         add ->    eps [leaf]       # 出栈 eps, 叶子节点不压栈
sigma -> mean_2 -> ['B']            # 出栈 sigma, 压栈 B
B ->     square -> ['A']            # 出栈 B, 压栈 A
A ->     sub ->    ['x', 'mu']***   # 出栈 A, 压栈 x, mu
mu ->    mean_1 -> ['x']            # 出栈 mu, 压栈 x
         mean_1 -> x [leaf]         # 出栈 x（来自mu）, 叶子节点不压栈
         sub ->    x [leaf]         # 出栈 x（来自A）, 叶子节点不压栈
A ->     sub ->    ['x', 'mu']***   # 出栈 A, 压栈 x, mu。这个 A 是从第一次压栈进来的
mu ->    mean_1 -> ['x']            # 出栈 mu, 压栈 x
         mean_1 -> x [leaf]         # 出栈 x（来自mu）, 叶子节点不压栈
         sub ->    x [leaf]         # 出栈 x（来自A）, 叶子节点不压栈
```

可以看到一共有四次 `-> x` 出现，说明四个分支都走到了起点。其中，后面带有 *** 的行，表示需要做按行累加的操作，把输出的梯度从 $m\times n$ 变成 $1\times n$，包括 $y_i \to D$ 和 $A \to \mu_B$ 两个反向计算步骤。

请注意，在本例中代码实现只是一个概念验证，它虽然可以用于神经网络训练，但是效率很低，所以在后续的章节中会使用其它更直接的方式来做训练。
