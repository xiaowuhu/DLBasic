
## 2.3 得到参考值【电子资源】

在使用神经网络计算 $w、b$ 的值之前，我们需要在心里先有个底，这两个值的标准答案是多少？这样才能评估神经网络的训练效果。所以我们先使用最小二乘法来的得到数学解析解。并非所有问题都可以使用 LSM 来得到解，它有一些适用条件，请读者自行参考其它资料。

### 2.3.1 最小二乘法

最小二乘法，也叫做最小平方法（least square method，LSM），它通过最小化误差的平方和来寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小二乘法来表达。

线性回归试图学得：

$$z_i=w \cdot x_i+b \tag{2.3.1}$$

使得：

$$
z_i \simeq y_i 
\tag{2.3.2}
$$

其中，$x_i$ 是样本特征值，$y_i$ 是样本标签值，$z_i$ 是模型预测值。

如何学得 $w$ 和 $b$ 呢？均方差 MSE 是回归任务中常用的手段：

$$
loss = \frac{1}{2m}\sum_{i=1}^m(z_i-y_i)^2 = \frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2 \tag{2.3.3}
$$

如果想让 $loss$ 误差函数的值最小，通过对 $w$ 和 $b$ 求导，再令导数为 $0$（到达最小极值），就是 $w$ 和 $b$ 的最优解。

$$
\begin{aligned}
\frac{\partial{loss}}{\partial{w}} &=\frac{\partial{(\frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2)}}{\partial{w}}
\\
&= \frac{1}{m}\sum_{i=1}^m(y_i-wx_i-b)(-x_i) 
\end{aligned}
\tag{2.3.4}
$$

$$
\begin{aligned}
\frac{\partial{loss}}{\partial{b}} &=\frac{\partial{(\frac{1}{2m}\sum_{i=1}^m(y_i-wx_i-b)^2)}}{\partial{b}} 
\\
&=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i-b)(-1) 
\end{aligned}
\tag{2.3.5}
$$

令式（2.3.4）、（2.3.5）为 $0$，最终可以解得：

$$
w = \frac{m\sum_{i=1}^m x_i y_i - \sum_{i=1}^m x_i \sum_{i=1}^m y_i}{m\sum_{i=1}^m x^2_i - (\sum_{i=1}^m x_i)^2} 
\tag{2.3.6}
$$

$$
b= \frac{1}{m} \sum_{i=1}^m(y_i-wx_i)=\bar{Y}-w\bar{X} 
\tag{2.3.7}
$$

而事实上，式（2.3.6）有很多个变种，大家会在不同的资料里看到不同版本，往往感到困惑，比如下面两个公式也是正确的解：

$$
w = \frac{\sum_{i=1}^m y_i(x_i-\bar x)}{\sum_{i=1}^m x^2_i - (\sum_{i=1}^m x_i)^2/m} 
\tag{2.3.8}
$$

$$
w = \frac{\sum_{i=1}^m x_i(y_i-\bar y)}{\sum_{i=1}^m x^2_i - \bar x \sum_{i=1}^m x_i} 
\tag{2.3.9}
$$

因为很多人不知道这个神奇的转换公式，利用这个性质可以得到上面对 $w$ 的不同表达：

$$
\begin{aligned}
\sum_{i=1}^m (x_i \bar y) &= \bar y \sum_{i=1}^m x_i =\frac{1}{m}(\sum_{i=1}^m y_i) (\sum_{i=1}^m x_i) \\
&=\frac{1}{m}(\sum_{i=1}^m x_i) (\sum_{i=1}^m y_i)= \bar x \sum_{i=1}^m y_i \\
&=\sum_{i=1}^m (y_i \bar x) 
\end{aligned}
\tag{2.3.10}
$$

### 2.3.2 代码实现

我们下面用【代码：H2_3_LeastSquare.py】来实现以上的计算过程：

#### 1. 计算 $w$ 值

```Python
# 式(2.3.8)
def method1(X,Y,m):
    x_mean = X.mean()
    p = sum(Y*(X-x_mean))
    q = sum(X*X) - sum(X)*sum(X)/m
    w = p/q
    return w

# 式(2.3.9)
def method2(X,Y,m):
    x_mean = X.mean()
    y_mean = Y.mean()
    p = sum(X*(Y-y_mean))
    q = sum(X*X) - x_mean*sum(X)
    w = p/q
    return w

# 式(2.3.6)
def method3(X,Y,m):
    p = m*sum(X*Y) - sum(X)*sum(Y)
    q = m*sum(X*X) - sum(X)*sum(X)
    w = p/q
    return w
```

由于有函数库的帮助，我们不需要编程实现`sum()`, `mean()`这些基本函数。

#### 2. 计算 $b$ 值

```Python
# 式(2.3.7)
def calculate_b_1(X,Y,w,m):
    b = sum(Y-w*X)/m
    return b

# 式(2.3.7)
def calculate_b_2(X,Y,w):
    b = Y.mean() - w * X.mean()
    return b
```

#### 3. 计算结果

下面计算本章中的 【数据：train2.txt】中的样本的 $w、b$ 值：

```
---- 最小二乘法计算 train2.txt 中的 w,b ----
w1=2.511870, b1=52.687349
w2=2.511870, b2=52.687349
w3=2.511870, b3=52.687349
```
用以上几种方法，最后得出的结果都是一致的，可以起到交叉验证的作用。结果是 $w \approx 2.51，b \approx 52$，即，房屋每平米单价约为 2.51 万元，每套房屋附加费用 52.69 万元。
