
## 2.6 多样本梯度下降法

在前面的代码中，我们一直使用单样本计算来实现神经网络的训练过程，但是单样本计算有一些缺点：

- 前后两个相邻的样本很有可能会对反向传播产生相反的作用而互相抵消。假设样本1造成了误差为 $0.5$，$w$ 的梯度计算结果是 $0.1$，紧接着样本2造成的误差为 $-0.5$，$w$ 的梯度计算结果是 $-0.1$，那么前后两次更新 $w$ 就会产生互相抵消的作用；
- 在样本数据量大时，逐个计算会花费很长的时间。由于我们在本例中样本量不大（100个样本），所以计算速度很快，觉察不到这一点。在实际的工程实践中，动辄10万甚至100万的数据量，轮询一次要花费很长的时间。

如果使用多样本计算，就要涉及到矩阵运算了，而所有的深度学习框架，都对矩阵运算做了优化，会大幅提升运算速度。打个比方：如果100个样本，循环计算一轮需要1秒的话，那么把100个样本打包成矩阵，做一次计算也许只需要0.1秒。

下面我们来看看多样本运算会对代码实现有什么影响，假设我们一次用三个样本来参与计算，每个样本只有1个特征值。

### 2.6.1 前向计算

### 2.6.2 损失函数

### 2.6.3 反向传播
