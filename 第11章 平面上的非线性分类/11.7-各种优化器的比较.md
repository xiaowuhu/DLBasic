
## 11.7 各种优化器的比较

下面我们比较一下各种优化器在 Beale 函数上的表现，它的表达式如式（11.7.1）：

$$
f(x) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2+(2.625-x_1+x_1x_2^3)^2
\tag{11.7.1}
$$

我们设置起始点为 $(1, 1.5)$，而这个函数的全局最小值在 $(3, 0.5)$ 处，只要误差小于 $1 \times 10^{-2}$ 就算是达到了终点。

### 11.7.1 SGD

见图 11.7.1。学习率大一些的话，会较快到达终点，但是比 0.02 再大一些的话，就有可能出现震荡，因为起点的梯度较大。后期的三条线都混在一起了，因为那里的梯度越来越小。

<img src="./img/opt_sgd.png" width=480>

图 11.7.1 SGD 在 Beale 函数上的表现


### 11.7.2 SGDM

见图 11.7.2。较大的动量如 0.8，在梯度较大的初期会带来较大的惯性，以至于在左下角绕了一个大弯子，但是在学习率相同（0.01）的情况下，后期可以帮助它尽快抵达终点。

<img src="./img/opt_momentum.png" width=480>

图 11.7.2 SGDM 在 Beale 函数上的表现


### 11.7.3 AdaGrad

见图 11.7.3。在本问题上，AdaGrad 的表现和 SGD 差不多，设置大的学习率也不会产生什么不良的影响，也许可以用于做神经网络训练初期的预热。

<img src="./img/opt_adagrad.png" width=480>

图 11.7.3 AdaGrad 在 Beale 函数上的表现

### 11.7.4 RMSProp

见图 11.7.4。较小的衰减速率 $\alpha$ 会使得其在终点附近产生较大的震荡，如 0.8 或 0.7，而 0.95 就会基本平稳地抵达最小值附近。

<img src="./img/opt_rmsprop.png" width=480>

图 11.7.4 RMSProp 在 Beale 函数上的表现

### 11.7.5 Adam

见图 11.7.5。由于 Adam 具有 SGDM 的惯性，所以它会“曲线救国”，像一条蛇一样逼近它的猎物。内部参数设置使得它的衰减程度较大，因此较大的学习率会帮助它尽快抵达终点。

<img src="./img/opt_adam.png" width=480>

图 11.7.5 Adam 在 Beale 函数上的表现


### 11.7.6 五种优化算法的比较

见图 11.7.6，把每种算法的最优参数的效果拿出来做一个比较。其中，AdaGrad 和 RMSProp 的轨迹几乎重叠，因为 RMSProp 的 $\alpha=0.95$，越接近 1，二者就越类似。但是随着步数的增加，RMSProp 会更快地到达终点。

<img src="./img/opt_compare.png" width=480>

图 11.7.6 五种优化算法的比较

下面是它们的打印输出，可以看到 Adam 一枝独秀，虽然绕绕绕去的，但是目标非常明确，只花了 85 步就抵达终点。其次是 RMSProp，用了 92 步。而 SGDM 在算法简单的前提下获得了 120 步的成绩已经非常好了，更何况它在左下角绕了一个大弯子。

```
stop at step 378, SGD(0.02)
stop at step 120, ('SGDM', 0.8)(0.01)
stop at step 92, ('RMSProp', 0.95)(0.20)
stop at step 376, AdaGrad(0.90)
stop at step 85, Adam(0.40)
```

运行【代码：H11_6_Optimizer_Compare.py】可以得到上述彩色图。
