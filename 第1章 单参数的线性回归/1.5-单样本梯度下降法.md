
## 1.5 单样本梯度下降法

下面我们使用梯度下降法解决第 1.1 节中提出的问题，每次计算时只使用一个样本，所以叫做单样本梯度下降法。

### 1.5.1 公式推导

首先列出单样本的计算公式，$x_i$ 是样本单例，$w$ 是标量权重，$z_i$ 是预测值：

$$
z_i = w \cdot x_i 
\tag{1.5.1}
$$



下面是单样本的损失函数，$w$ 是我们的目标参数，而不是样本 $x$。$y_i$ 是与样本 $x_i$ 对应的标签值：

$$
loss = \frac{1}{2} (z_i-y_i)^2 
\tag{1.5.2}
$$

根据链式求导法则计算损失函数梯度：

$$
\frac{\partial loss}{\partial{w}}=\frac{\partial loss}{\partial{z_i}} \frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i 
\tag{1.5.3}
$$

梯度下降迭代公式：

$$
w = w - \eta \cdot \nabla loss = w - \eta \cdot (z_i-y_i)x_i 
\tag{1.5.4} 
$$

### 1.5.2 代码实现

用代码实现上述过程【代码：H1_5_GD.py】：

```python
def single_sample_gd(train_data, eta):
    w = 0
    for i in range(train_data.shape[1]):  # 遍历所有样本
        # get x and y value for one sample
        x_i = train_data[0, i]
        y_i = train_data[1, i]
        # 式（1.5.1）
        z_i = x_i * w
        # 式（1.5.3）
        d_w = (z_i - y_i) * x_i
        # 式（1.5.4）
        w = w - eta * d_w
    return w
```
然后加载样本数据，调用上述函数。

```python
if __name__ == '__main__':
    file_name = "train.txt"
    train_data = load_data(file_name)
    w = single_sample_gd(train_data, 0.1)
    print(w)
```
得到的结果很令人失望：

```
w=-1.18916e+29
```
这与我们在 1.1 节中预估的 $w=2.15$ 相去甚远。哪里出了问题呢？

首先怀疑 $w=0$ 的初始值是否有影响？经过试验后，发现即使初始化 $w=2.14$ 也得不到正确结果。

回想第 1.4 节中的式（1.4.1），当 $\eta=1$ 时，下一个迭代会在极值点附近左右乱跑，原因是步长值太大所致。既然 0.1 不行，我们可以多试试其它值。

```python
    for eta in [0.1, 0.01, 0.001, 0.0001]:  # 测试不同的步长值
        w = single_sample_gd(train_data, eta)
        print("eta=%f, w=%f" %(eta, w))
```
得到如下结果：
```
eta=0.100000, w=9916973800777358146164228096.000000 # 太大
eta=0.010000, w=856865788969505664.000000           # 太大
eta=0.001000, w=15717097.592734                     # 太大
eta=0.000100, w=2.148374                            # 合适
```
我们惊喜地发现，当 $\eta=0.0001$ 时，得到 $w=2.148374$ 的结果，和预估值非常接近！

### 1.5.3 为什么需要很小的步长值？

在第 1.4 节的例子中，设置步长值为 0.1 并迭代多次就可以无限逼近极值点，在这里中为什么不行呢？观察式（1.5.4）的具体数值，$w = w - \eta \cdot (z_i-y_i)x_i$，其中 $x_i、y_i、z_i$ 的值都是 3 位数，相乘一次后一般会变成 6 位数，乘以 0.1 后也是 5 位数，这与 $w \approx 2.14$ 的数量级相去甚远，所以要至少乘以 0.0001 才有可能达到要求。

### 1.5.4 算法改进

进一步思考，$w=2.148374$ 这个值足够精确吗？是否还有改进的空间？

回顾具体实现代码，发现对所有样本只做了一次遍历。假设最后一个样本噪音很大，标签值偏移得离谱，那么仅一次遍历就会得到有偏的估计。所以需要下面三点改进：

（1）每次遍历样本前把训练样本的顺序打乱；

（2）多轮遍历样本；

（3）使用更小的步长值来降低对样本噪音的敏感度。

【代码：H1_5_GD_Improve.py】完成了打乱顺序的工作：

```python
# 打乱样本顺序，但是保证 x-y 的对应关系
def shuffle_data(train_data):
    idx = np.random.permutation(train_data.shape[1])
    new_train_x = train_data[0][idx]
    new_train_y = train_data[1][idx]
    return new_train_x, new_train_y
```

上述代码不但保证特征值和标签值的对应关系，还把训练数据分成了 $X、Y$ 两个部分，方便后面使用。下面的代码改进了算法，总共遍历 1000 轮样本数据，每次都做随机打乱，完成了第二点改进。

```python
if __name__ == '__main__':
    file_name = "train.txt"
    train_data = load_data(file_name)
    print("----- 相同的步长值 0.0001, 分别运行 4 次 ----- ")
    eta = 0.0001
    for i in range(4):
        w = 0
        for j in range(1000):
            new_train_x, new_train_y = shuffle_data(train_data)
            w = single_sample_gd(new_train_x, new_train_y, w, eta)
        print("w=%f" %(w))
```
但是运行多次后，发现每次的结果都不一样。如：

```
----- 相同的步长值 0.0001, 分别运行 4 次 ----- 
w=1.996433
w=2.022359
w=2.394225
w=1.691489
```

这是为什么呢？这还要从步长值 $\eta$ 下手分析，可以尝试把它改得再小一些：

```python
    print("----- 尝试不同的步长值 ----- ")
    for i in range(3):
        print("第%i次:" %(i+1))
        for eta in [0.0001, 0.00001, 0.000001]:
            w = 0
            for j in range(1000):
                new_train_x, new_train_y = shuffle_data(train_data)
                w = single_sample_gd(new_train_x, new_train_y, w, eta)
            print("eta=%f,w=%f" %(eta, w))
```

运行三次，得到如下结果：

```
               第1次          第2次            第3次
-------------+-----------------------------------------             
eta=0.000100 | w=1.966930     w=1.824893      w=2.607305   # 上下漂移很大
eta=0.000010 | w=2.126110     w=2.110993      w=2.132828   # 漂移幅度减小
eta=0.000001 | w=2.139811     w=2.140026      w=2.140112   # 稳定
```
可以看到：
- 当 $\eta=0.000100$ 时，$w$ 的值还是上下漂移很大；
- 当 $\eta=0.000010$ 时，$w$ 的值上下漂移幅度减小；
- 当 $\eta=0.000001$ 时，$w$ 的值基本稳定在 2.14 附近，那么这就是比较精确的值了。

为什么会这样呢？这说明样本的顺序对训练的影响非常大，所以只能使用很小的步长值来抑制噪音。由此可见，步长值的选择是多么的重要。我们将在第 2 章中用其它方法来解决这个问题。
