
### 思考与练习

1. 在 12.3 中，使用 0.9 参数做验证集分割，重新训练网络。
2. 试验不同的优化器在 MNIST 上的作用，调整学习率，并对效果做比较，主要是观察收敛速度。
3. 使用不同的批大小做训练，比较效果。
4. 使用更深的网络解决 MNIST 问题。
5. 延长训练 12.4 节中的例子，看看 loss 值是否会收敛。保存权重参数，并最后运行【代码：H12_4_Toy_Test.py】观察各阶段的值。
6. 建立 MNIST 分类结果的混淆矩阵。
7. 除了批量归一化 BN 以外，还有组归一化（group normalization）、层归一化（layer normalization）、实例归一化（instance normalization）、可交换归一化（switchable nornmalization），请读者自己学习理解。
8. 批量归一化的原理是什么？应该放在网络中的什么位置？
9. 集成学习有哪些方法？
10. 数据并行计算如何实现？
