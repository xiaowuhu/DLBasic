
## 15.7 深度可分离的 MobileNet

在世界范围内掀起的云计算（cloud computing）热潮还在持续，而且 5G 等无线网络速度也达到了一个新的高度，但是这仍然不能解决在移动端的计算问题（edge computing），总不能把所有移动设备上捕捉的图像都传送到云端去进行识别吧。而在移动端，内存大小已经不是什么问题了，关键是要考虑电量使用的问题，这就要求尽量减少运算量。

### 15.7.1 两种卷积的比较

#### 1. 普通卷积

普通的卷积，如果从 3 通道变成 64 通道，其参数为 3×3×3×64=1728，如图 15.7.1 左图所示，这里 stride=1 以使得图片尺寸不变。

<img src="./img/MobileNet_1.png" width=780>

图 15.7.1 普通卷积与深度可分离卷积的参数比较

普通卷积的计算开销，在 15.5.2 节中已经总结过，如果只考虑乘法的话为 $C_{in}K^2HWC_{out}$。相对于全连接层来说，卷积已经是比较稀疏的了，但是仍然可以有挖掘的空间，下面看看深度可分离卷积。

#### 2. 深度可分离卷积

一个普通的 3×3 矩阵，其实可以从空间上分解为一个 3×1 的矢量与一个 1×3 的矢量相乘：

$$
\begin{pmatrix}1 & 2 & 3 \\ 0 & 0 & 0 \\ 2 & 4 & 6 \end{pmatrix}=
\begin{pmatrix}1 \\ 0 \\ 2 \end{pmatrix} \times
\begin{pmatrix}1 & 2 & 3 \end{pmatrix}
$$

那么普通卷积也可以做类似的分解计算，如图 15.7.1 的右图所示。

首先使用三个 3×3 的卷积核对每个输入通道单独做卷积运算，输出结果仍为三个通道，**并不相加**。这一步叫做**深度卷积**（depthwise），参数量为 3×3×3=27，计算量为 $C_{in}K^2HW$。

其次使用 1×1 的卷积进行升维，叫做**逐点卷积**（pointwise），参数量为 3×1×1×64=192，计算量为 $C_{in}HWC_{out}$，因为 $K=1$。

二者计算量相加为 $C_{in}K^2HW+C_{in}HWC_{out}$，与普通卷积比较：

$$
\frac{C_{in}K^2HW+C_{in}HWC_{out}}{C_{in}K^2HWC_{out}}=\frac{1}{C_{out}}+\frac{1}{K^2}
\tag{15.7.1}
$$

一般 $C_{out}$ 的取值比较大，为 64、128、256、512 等，而 $K$ 值一般等于 3，所以深度可分离卷积的计算量约为普通卷积的 $\frac{1}{9} \sim \frac{1}{8}$，这使得在移动设备上做卷积运算对电池的损耗大大降低。

联合使用深度卷积和逐点卷积，可以达到和普通卷积相同的输出维度 64×28×28。由于使用了两组卷积核，并且第一层卷积时不混合通道，所以叫做**深度可分离卷积**，总参数量为 27+192=219，比普通卷积的参数量少很多。


### 15.7.2 认识 MobileNet 网络

MobileNet 在 2017 年提出，针对移动设备资源有限的问题，采用了深度可分离卷积模块搭建网络，实现了高效且轻量级的模型，并且迅速衍生了三个版本。图 15.7.2 是普通卷积块和深度可分离卷积块的结构比较，在 Conv(depth wise) 和 Conv(point wise) 之间需要有 BN/ReLU 的标准组合。

<img src="./img/MobileNet_3.png" width=400>

图 15.7.2 普通卷积与深度可分离卷积的结构比较

表 15.7.1 是 MobileNet 的网络结构，其中，Conv 表示普通卷积，其中包含 1×1 的逐点卷积；Conv dw 表示深度卷积；s1、s2 表示步长。

表 15.7.1 MobileNet的网络结构（来自原论文并略有改动）

<img src="./img/MobileNet_2.png" width=420>

从结构中可以看到，它没有使用最大池化层来减少图片尺寸，而是在 `Conv dw` 层用步长为 2 来代替。另外，和 ResNet 一样，由于模型小，所以不使用 Dropout 等正则化手段以及数据增强手段，这与训练大模型的思路不同。表 15.7.2 与同级别的其它网络比较了准确率、计算量、参数量，MobileNet 只有 4.2M 参数和 569M 计算量，远远小于其它模型，但是在 ImageNet 的评估准确率上与其它模型接近，说明它很好地做到了性能与尺寸的平衡。

表 15.7.2 MobileNet 与其它同级别网络的比较

|模型|ImageNet准确率|计算量|参数量|
|-|:-:|-|-|
|普通卷积|71.7%|4866M|29.3M|
|GoogLeNet|69.8%|1550M|6.8M
|VGG16|71.5%|15300M|138M|
|MobileNet|70.6%|569M|4.2M|

### 15.7.3 MobileNet 在 CIFAR-100 上的表现

下面的代码中实现了深度可分离卷积模块，注意`self.depth_conv`中的卷积使用 `groups=in_ch` 参数表示不混合通道，而 `self.point_conv` 中的卷积不需要特殊设置：

```python
class DepthwiseConv(nn.Module):
    def __init__(self, in_ch, out_ch, strides=1):
        super(DepthwiseConv, self).__init__()
        self.depth_conv = nn.Sequential(  # 深度分离卷积
            nn.Conv2d(in_ch, in_ch, kernel_size=3, stride=strides, padding=1, groups=in_ch),
            nn.BatchNorm2d(in_ch),
            nn.ReLU(inplace=True),
        )
        self.point_conv = nn.Sequential(  # 1x1 卷积
            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        out = self.depth_conv(x)
        out = self.point_conv(out)
        return out
```

接下来只需要按照表 15.7.1 中的说明堆叠 13 个模块即可，注意当有 `/s2` 标记时，需要设置 strides=2，如：
```python

class MobileNet(nn.Module):
    def __init__(self, num_classes=100):
        super(MobileNet, self).__init__()
        ... # input
        self.dp1_6 = nn.Sequential(  # 1~6 层
            DepthwiseConv(32, 64, strides=1),   # 1
            DepthwiseConv(64, 128, strides=1),  # 2, 本来是s=2,改为s=1
            DepthwiseConv(128, 128, strides=1), # 3
            DepthwiseConv(128, 256, strides=2), # 4, 保留原始设置s=2
            DepthwiseConv(256, 256, strides=1), # 5
            DepthwiseConv(256, 512, strides=2), # 6, 保留原始设置s=2
        )
        self.dp7_11 = nn.Sequential(  # 7~11 层
            DepthwiseConv(512, 512, strides=1), # 7
            ... # 8,9,10,11,12 同上
        )
        self.dp12_13 = nn.Sequential(  # 12~13 层
            DepthwiseConv(512, 1024, strides=2)  # 12, 保留原始设置s=2
            DepthwiseConv(1024, 1024, strides=1),# 13  
        )
        ... # output
```

但是由于 CIFAR-100 的图片尺寸为 32×32，所以我们在表 15.7.1 中的 `input` 和 `dp2` 两层设置步长为 1，不做尺寸降维，这样到最后的卷积输出还可以有 4×4 的特征进入平均池化层。

在数据增强方面，使用了以下几种方法：

```python
    ...
    transforms.RandomHorizontalFlip(p=0.1),
    transforms.RandomVerticalFlip(p=0.1),
    transforms.RandomRotation(degrees=(-10,10)),
    transforms.RandomAffine(0, translate=(0.1,0.1)),
    transforms.RandomPerspective(p=0.1),
    transforms.RandomErasing(p=0.1),
    transforms.RandomCrop(32, padding=8),
    ...
```

在训练策略上，批量 64，用 Adam 优化器，学习率设置为 0.01，每 5 轮降低 10%，共 100 轮，得到一个初步的结果。然后使用 SGD 优化器细调，动量为 0.9，学习率为 0.0001，权重衰减为 5e-4，最终得到测试集上 72.73% 的准确率。

### 15.7.4 两个附加因子

#### 1. 宽度因子 $\alpha$

MobileNet 本身的网络结构已经比较小并且执行延迟较低，但为了适配更定制化的场景，MobileNet 提供了称为宽度因子（width multiplier）的超参数用于调整。通过宽度因子 $\alpha \in (0,1]$，可以调整神经网络中间产生的特征的大小，调整的是特征数据通道数大小，从而调整了运算量的大小，从：

$$
C_{in}K^2HW+C_{in}HWC_{out} 
$$

变成：

$$
\alpha C_{in} \cdot K^2 \cdot H \cdot W+\alpha C_{in} \cdot H\cdot W \cdot \alpha C_{out}
\tag{15.7.2}
$$

$\alpha$ 常用的配置为 1、0.75、0.5、0.25，等于 1 时就是标准的 MobileNet。举例来说，当取值为 0.5 时，表 15.7.1 中的所有关于通道的数字都要乘以 0.5，比如第一行的 3×3×3×32 变成 3×3×3×16，第三行 1×1×32×64 变成 1×1×16×32 等。

当然，通道减少必然会带来性能的损失，表 15.7.3 比较了四种取值时的各种数据。

表 15.7.3 四种宽度因子的模型性能比较

|$\alpha$|ImageNet准确率|计算量|参数量|
|-|-|-|-|
|1.0|70.6%|569M|4.2M|
|0.75|68.4%|418M|2.6M|
|0.5|63.7%|149M|1.3M|
|0.25|50.6%|41M|0.5M|

可以看到当输入分辨率固定为 224×224 时，随着宽度因子的减少，模型的计算量和参数越来越小。从上表可以看到， 0.25 MobileNet 的正确率比标准版 1.0 MobileNet 低 20%，但计算量和参数量几乎只有标准版 10%！对于计算资源和存储资源都十分紧张的移动端平台，可以通过宽度因子调节网络的餐数量是非常实用的，在真正使用时我们可以按需调整宽度因子达到准确率与性能的平衡。

#### 2. 分辨率因子

MobileNet 还提供了另一个超参数分辨率因子（resolution multiplier）供用户自定义网络结构，用 $\beta \in (0,1]$ 来指代，作用于每一个模块输入尺寸的约减因子，简单来说就是将输入数据以及由此在每一个模块产生的特征图都变小了，

$$
\alpha C_{in} \cdot K^2 \cdot \beta H \cdot \beta W+\alpha C_{in} \cdot  \beta H\cdot  \beta W \cdot \alpha C_{out}
\tag{15.7.3}
$$

在 $\alpha$ 取值固定时，分辨率因子并没有改变模型的参数量，即模型的大小没有变化，但是通过降低输入图片的分辨率使得计算量减少，如表 15.7.4 所示。

表 15.7.4 四种分辨率因子的模型性能比较

|$\alpha$|$\beta$|输入图片分辨率|ImageNet准确率|计算量|参数量|
|-|-|-|-|-|-|
|1.0|1.0|224|70.6%|569M|4.2M|
|1.0|6/7|192|69.1%|418M|4.2M|
|1.0|5/7|160|67.2%|290M|4.2M|
|1.0|4/7|128|64.4%|186M|4.2M|

当 $\beta=1$ 时，输入图片的分辨率为 224×224，卷积后的图像大小变化为：224×224、112×112、56× $\beta=6/7$ 时，输入图片的分辨率为192×192，卷积后各层特征图像大小变化为：192×192、96×96、48×48、24×24、12×12、6×6。

卷积特征图像的大小变化不会引起参数量的变化，只改变模型计算量。224 分辨率模型测试 ImageNet 数据集准确率为70.6%，192 分辨率的模型准确率为 69.1%，但是计算量减少了 151M，对移动平台计算资源紧张的情况下，同样可以通过分辨率因子调节网络输入特征图的分辨率，做模型精度与计算量的取舍。

### 15.7.5 MobileNet 的后续发展

随后几年，MobileNet 从 V1 发展到了 V2、V3。

在 V2 版本中，引入了线性瓶颈结构（linear bottleneck）和反向残差结构优化了网络，使得网络层次可以更深，但是模型体积却更小。所谓反向残差指的是升降维的顺序与图 15.5.7 中相反，变成先升维后降维。另外还是用了 ReLU6 激活函数，其表达式为 $\min(\max(0,x),6)$，即，在 0~6 之间用线性激活，大于 6 时被砍掉，相当于梯度剪裁。

V3 版本在深度分离卷积块中使用了 5×5 的卷积代替 3×3 卷积，同时引入了挤压和激励（squeeze-and-excitation，SE）模块，使得有效的权重大，无效的权重小；还使用 Hardswish 激活函数代替部分 ReLU 激活函数，其表达式为：

$$
\text{Hardwish}(x) = \begin{cases}
0, & x \le -3
\\
x(x+3)/6, & -3 < x < 3
\\
x, & x \ge 3
 \end{cases}
$$

其图像见图 15.7.4。

<img src="./img/MobileNet_4.png" width=420>

图 15.7.4 Hardwish激活函数图像
