
## 15.6 跳跃连接的 ResNet

ResNet 在深度学习中的地位非常重要，自从提出了 ResNet 之后，神经网络的深度比以前大大增加了，有利于提取高维特征，同时减少了梯度消失与网络退化的情况。我们先看看梯度消失和网络退化的具体表现是什么。

### 15.6.1 梯度消失与网络退化

#### 1. 梯度消失

先看两个不同的网络结构图，见图 15.6.1。

<img src="./img/ResNet_2.png" width=420>

图 15.6.1 顺序连接和跳跃连接的网络

图 15.6.1 中，左图是一个到目前为止常见的顺序连接结构，为了增加网络的深度以提高表现能力。但是它的最大问题就是当网络很深时，反向传播的力度会越来越小，尽管使用了 ReLU 激活函数也是如此。可以看图 15.6.2 中的虚线所示，靠左侧的 FC5 的输出梯度很大，到了右侧的 FC1 的梯度已经小了好几个数量级了（纵坐标为对数坐标）。

为了具备可比较性，我们在各层 FC 中使用了相同尺寸的参数（都是8×8），而且参数也相同，相同的输入 $X$，相同的标签值 $Y$ 做反向传播。

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(8, 8)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(8, 8)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(8, 8)
        self.relu3 = nn.ReLU()
        self.fc4 = nn.Linear(8, 8)
        self.relu4 = nn.ReLU()
        self.fc5 = nn.Linear(8, 8)
```

由于各层参数量相同，为了比较梯度的大小，简单地计算输入输出梯度的模即可 `grad_norm = torch.norm(grad)`。如果参数量不同，则需要计算模的平方除以参数个数，即 $\sqrt{(g_1^2+\cdots+g_n^2)/n}$，就可以获得可比的值。因为模的平均数并不可比，模的平方的平均数可比，比如下面的例子：

```python
a = torch.ones((10))  # 10 个 1
b = torch.ones((5))   # 5 个 1
torch.norm(a)/10 != torch.norm(b)/5  # 3.1623/10 != 2.2361/5, 模的平均数不可比
torch.norm(a)**2/10 == torch.norm(b)**2/5  # 1 == 1 # 模的平方的平均数可比
```

<img src="./img/ResNet_1.png" width=420>

图 15.6.2 有跳跃连接和无跳跃连接的反向传播力度

在图 15.6.1 右图中，在 FC1/ReLU1 的输出引出一个分支，直接到 FC4 的输入，称为跳跃连接。此时再做反向传播时，FC1/ReLU1 会收到来自两个分支的梯度，把它们相加后展示在图 15.6.2 的实线上，可以看到 FC1 的传入梯度一下提升了一个数量级。以上试验是在全连接网络中进行的，在卷积网络中会有同样的结果。

#### 2. 网络退化

在持续增加网络层数的过程中，训练准确率逐渐趋于稳定，很难再提高。此时如果强行继续增加网络层数，想借此提高网络能力，准确率和误差反而会出现下降的现象，这种下降**不是**由过拟合造成的，被称作**退化**（degradation）。理论上讲，较深的模型不应该比它的子集（较浅的模型）更差，因为较深的模型是较浅的模型的超空间。较深的模型可以这样得到：先构建较浅的模型，然后添加很多恒等映射（identity）的网络层。如图 15.6.3 所示。

<img src="./img/ResNet_3.png" width=420>

图 15.6.3 浅层网络增加恒等映射变成“深层”网络

实际上真正的较深模型后面添加的不是恒等映射，而是一些非线性层，因此而出现了退化，也表明通过多个非线性层来近似甚至超过恒等映射可能是困难的。这种现象在理论上无法解释，这只能怀疑到梯度下降的优化手段上，反映出结构相似的模型其优化难度是不一样的，且难度的增长并不是线性的，越深的模型越难以优化。


### 15.6.2 认识 ResNet

#### 1. 基础残差块

在历年 ImageNet 挑战赛中获得优胜的神经网络都很深（从 16 层到 30 层不等），但是深度模型会遇到上述的两种困境。有两种解决思路，一种是调整求解方法，比如更好的初始化、更好的梯度下降算法等；另一种是调整模型结构，让模型更易于优化，而改变模型结构实际上是改变了损失函数的形态。ResNet （residual network）就是用第二种思路提出了残差块（residual block）的概念，这种设计允许信息直接在跨层间传递，使得构建更深的网络成为可能。如图 15.6.4 所示。

<img src="./img/ResNet_4.png" width=300>

图 15.6.4 基础残差块结构

假设在图 15.6.4 的输出中需要得到 $y=f(x)$ 的映射，可以把它拆解为 $y=F(x)+x$ 的形式，其中 $F(x)=f(x)-x$，那么让网络学习 $F(x)$ 可得到的最终映射效果与 $f(x)$ 是相同的。这种方法可以得到的好处是优化求解 $F(x)$ 比较简单，至少比求解未知的 $f(x)$ 更简单。退一步讲，万一恒等映射是最优的，那么只需要让残差块中的各项参数为 0，就可以从右侧的 identity 路径来实现恒等映射，因为此时 $F(x)=0$。图中的 $\oplus$ 为 element-wise addition（元素对位相加），要求参与运算的 $F(x)$ 和 $x$ 的尺寸要相同。

因此，残差块就是使用两层的 3×3 卷积后接 BN，输出通道为 64 或其它值，中间用 ReLU 连接，其输出与 $x$ 相加后再做一次 ReLU 成为残差块的总输出。具体实现如下：

```python
class Residual_x(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.res_block = nn.Sequential(  # 残差块主分支
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, bias=False),
            nn.BatchNorm2d(out_ch)
        )
    def forward(self, x):
        output1 = self.res_block(x)
        output = F.relu(output1 + x)  # 按元素相加
        return output
```

#### 2. 降维/升维残差块

有了基础的残差块还不够，因为在块堆叠的过程中还希望不断地降维（减少图片尺寸）。如果在两个残差块之间加一层最大池化层的话，残差块的“短路”作用将被阻隔，所以需要把降维机制设计在残差块内部，如图 15.6.5 所示。

<img src="./img/ResNet_5.png" width=320>

图 15.6.5 降维残差块结构

假设从上层传递过来一批 64 通道的数据，尺寸为 56×56。第一个卷积层设置为输出 128 通道，同时 stride=2（即图 15.6.5 中符号 “/2” 的含义），则图片尺寸缩小了一倍变成 128×28×28。第二个卷积层不改变尺寸，则总输出为 128×28×28。此时右侧的“快捷通道”可以采用 1×1 卷积进行**通道升维和尺寸降维**，只需下面的代码即可实现。

```python
class ResBlock_conv1x(nn.Module):
    def __init__(self, in_ch, out_ch, strides=1):
        super().__init__()
        self.res_block = nn.Sequential( # 残差块主分支
            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_ch)
        )
        self.shortcut = nn.Sequential(  # 有升降维的分支连接
            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2, padding=0, bias=False),
            nn.BatchNorm2d(out_ch)
        )
    def forward(self, x):
        output1 = self.res_block(x)
        output2 = self.shortcut(x)
        output = F.relu(output1 + output2)
        return output
```        

#### 3. 搭建网络

现在可以玩“积木”堆叠游戏了，见图 15.6.6。

<img src="./img/ResNet_6.png" width=720>

图 15.6.6 ResNet18的网络结构

网络分成三大部分：

- 输入端，先做一个 `7x7 64 /2` 的卷积，再做一个最大池化，输入 $X$ 的形状变成了 64×56×56。这一设计延续了 VGG 和 GoogLeNet 的方法，在不丢失特征的前提下快速降维。
- 堆叠部分，四组残差块堆叠，每组两块，后三组中的每组第一个块做降维（虚线所示），最后数据形状为 512×7×7。
- 输出端，延续了 GoogLeNet 的设计，先做平均池化，把 7×7 的特征变成一个值，然后只经过一层 FC 层就进入分类函数，这样做可以大幅减少参数数量，总共为 11.2M，浮点计算量（floating point operations，FLOPs）为 $1.8\times10^9$。

关于 CNN 的 FLOPs 的计算有以下公式：

- 乘法：$C_{in}K^2$，其中，$C_{in}$ 是输入通道数，$K$ 是卷积核大小。
- 加法：$C_{in}K^2-1$。
- 偏置：加法 1 次。
- 以上小计：$2C_{in}K^2$ 次。
- 输出通道为 $C_{out}$，输出尺寸为 $H \times W$，则共计 $2C_{in}K^2HWC_{out}$ 次。

图 15.6.6 中共有 18 层网络组件需要学习参数，所以被称作 ResNet18，残差块配置数量为 [2,2,2,2]。根据不同配置，还有 ResNet34（[3,4,6,3]）、ResNet50（[3,4,6,3]）、ResNet101（[3,4,23,3]）、ResNet152（[3,8,36,3]） 等，后三种网络中的每组残差块为三个。以 ResNet50 为例，[3,4,6,3] 相加得 16，每组残差块三个，所以 16×3=48，再加上输入端的卷积和输出端的 FC，一共 50 层。

在大于 50 层的深度网络中，需要图 15.6.7 的模块。这个模块专为深层网络设计，可大大减少参数量：它先通过 1×1 卷积把 256 通道的数据降维到 64 通道，然后是 3×3 卷积，再接着用 1×1 卷积把 64 通道还原为 256 通道做升维，形成两头胖（256）中间瘦（64）的结构，所以被称作瓶颈（bottleneck）残差块。

<img src="./img/ResNet_7.png" width=320>

图 15.6.7 用于深层网络中的瓶颈残差块

总结 ResNet 的设计特点如下：

- 与普通堆叠网络相比，ResNet 多了很多“旁路”，其首尾圈出的部分构成一个残差块组（residual block）。
- ResNet 中，所有的部分都没有池化层，降采样是通过卷积的步长实现。
- 通过平均池化（average pooling）得到最终的特征，而不是通过全连接层。
- 每个卷积层之后都紧接着 BN，为了简化，图中没有标出。
- ResNet 结构非常容易修改和扩展，通过调整块内的通道数量以及堆叠的块数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。

### 15.6.3 ResNet18 在 CIFAR-100 上的表现

首先我们使用 ResNet18 的原始参数来进行试验。由于它是为 224×224 的图片尺寸而设计的，所以需要把 CIFAR-100 的图片从 32×32 放大到 224×224。尽管我们做了很多数据增强工作，在训练了 100 轮后在测试集上得到 67.8% 的准确率，不令人满意。

所以接下来要对 ResNet18 进行修改，主要发生在输入端部分的代码中【代码：H15_5_ResNet_Model.py】：
```python
    self.conv1 = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # 步长从2改成1
        nn.BatchNorm2d(64),
        nn.ReLU(inplace=True),
        #nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 取消最大池化
    )
```
首先是把第一个卷积的步长从 2 变成 1，这样图片就不会缩小。其次是取消最大池化操作，这样也能保持图片尺寸。这样一来，在进入输出端部分的全局平均池化前，仍然有 4×4 的特征图大小，比原始的设计 7×7 小一些，但是对于 32×32 的原始图片来说可以接受。

接下来是数据部分，主要的数据预处理链如下：

```python
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4856, 0.4409), (0.2673, 0.2564, 0.2762)),
        transforms.RandomHorizontalFlip(p=0.1),
        transforms.RandomRotation(degrees=(-10,10)),
        transforms.RandomAffine(0, translate=(0.1,0.1)),
        transforms.RandomErasing(p=0.1),
        transforms.RandomCrop(32, padding=4),
    ])),
```            
以下是训练的准备工作：
```python
def main(net:nn.Module, save_name:str, pretrained_model_name:str = None):
    batch_size = 64
    epoch = 100
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train_loader, test_loader = create_data_loader(batch_size)
    model:nn.Module = net.to(DEVICE)
    #加载预训练模型, 以避免死机、断电等情况发生
    if pretrained_model_name is not None:
        load_model(model, pretrained_model_name, DEVICE)
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1, weight_decay=5e-4)
    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 80], gamma=0.2)
    best = 5000 # 大于此值时才保存当前最佳模型
    train_model(epoch, model, DEVICE, train_loader, test_loader, 
                optimizer, train_scheduler, loss_func, save_name, best_correct=best)
```
一共 120 轮，被 $[30,60,80]$ 分成了四个阶段，第一阶段 0~30 轮学习率为 0.1，第二阶段 30~60 轮学习率为 0.02，第三阶段 60~80 轮学习率为 0.004，第四阶段 80~100 轮学习率为 0.0008。在第一阶段结束时可以得到 50% 左右的测试集准确率，第二阶段上升到 60%，第三阶段上升到 70%，第四阶段达到 75.63%。如果用指数衰减学习率的话，效果不如这种断崖式衰减好。

### 15.6.4 ResNet 的后续发展

在分析了残差连接的方式之后提出 ResNet V2，以此训练了 1001 层的模型。新旧对比有两个变化：

- 上一层的信息到下一层的进行直接的传递（没有 ReLU 激活层），使得信息传递更直接，可重复利用上一层的信息。文中作者做了很多不同的信息传递的尝试，效果都不佳；

- 尝试了不同的激活函数的组合方式，发现预激活（pre-activation）效果是最好的，也就是先激活，再用 BN。

ResNet V2 探索了在有效的信息传递的方式，可以训练更深的网络，效果也更好。而 Wide Residual Networks（WRN）指出网络可以训练的很深，但是往往精度的提升需要堆叠大量的层，使得训练时长加长；另一方面，ResNet 天然的假设允许一个残差块可以不学习任何东西，某些网络层实际上是浪费的。在此基础上作者提出了 WRN，减少深度，而增加宽度，使得每个残差块能学习到更加有效表示，指出即使 16 层的WRN也能达到更深网络的精度。WRN 在原始的特征上扩大 K 倍，同时也增加 Dropout 层提高泛化。

ResNeXt 在深度和宽度设计之外，从cardinality（基数）角度重新考虑模型的设计。ResNet 是采用重复的策略（repetition strategy），ResNeXt 采用的是分裂-转换-合并（split-transform-merge strategy），这个分裂的数量就是 cardinality。每个分裂的转换结构结构是相同的，但是学习是独立的，增加了学习特征的多样性，也是文章标题中提到的集成多个特征转换的含义所在。
